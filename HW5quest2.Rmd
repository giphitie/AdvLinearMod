---
title: "Question 1"
author: "Gifty Osei"
date: "2024-11-01"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(broom)
```

## 1b.

In general, a nonlinear trend can look fairly linear when data contain noise. For example, the following code plots data generated from $y = f(x1, x2)$ = $x_1 ^2 + x_2^2 + \epsilon$, where $\epsilon \sim N(0,0.02^2)$.

```{r, warning=FALSE, message=FALSE}
set.seed(390)
n <- 100
# simulate 20 values from uniform(0,1)
x1 = runif(n,1,20)
x2= runif(n,1,20)

y = x1^2+x2*x2+rnorm(20,sd=0.02)
plot(y~x1, main = " Marginal plot of y vs x1", xlab = "x1", ylab = "y")
plot(y~x2,main = " Marginal plot of y vs x2", xlab = "x2", ylab = "y")



```


 We can see clearly form the marginal plots showing a linear trend but the overall model that generated the y values is nonlinear in x. We can see that from the joint marginal plot that the are jointly nonlinear. We can see that the joint marginal plot shows and captured the quadratic pattern of the form of $Y$
 
```{r, warning=FALSE, message=FALSE, eval=FALSE}
library(plotly)
plot_ly(x = ~x1, y = ~x2, z = ~y, type = "scatter3d", mode = "markers",
        marker = list(size = 3)) %>%
  layout(scene = list(xaxis = list(title = "x1"),
                      yaxis = list(title = "x2"),
                      zaxis = list(title = "y")))
```
 

\newpage

## Question 2 

### 2a

```{r, warning=FALSE, message=FALSE}


# simulate data
n <- 100
x <- runif(n, 1, 10)  
beta_0 <- 3
beta_1 <- 2
sigma <- 3

# Generate y values based on the original model
epsilon <- rnorm(n, mean = 0, sd = sigma * x)
y <- beta_0 + beta_1 * x + epsilon

# transformations
y_prime <- y / x
x_prime <- 1 / x

# Check the variance of y_prime
var_y_prime <- var(y_prime)
print(sqrt(var_y_prime))

```

We can see that variance of $y^{'}$ is very close to the initial $\sigma$

### 2c

This is equivalent to OLS estimate because using the weights on the original data is like doing a $\frac{y}{x}$ transformation from part a. We can see from the coefficients that there is not much change although, intercept from OLS is now the slope for WLS and slope now intercept.

```{r, warning=FALSE, message=FALSE}
#  wls model
fit_wls <- lm(y ~ x, weights = 1 / x^2)

# OLS model
fit_ols_trans <- lm(y_prime ~ x_prime)

# Compare the coefficients
summary(fit_wls)$coefficients
summary(fit_ols_trans)$coefficients

```

## Question 3

```{r, warning=FALSE, message=FALSE}
library(faraway)
pander::pander(head(pipeline))

```


### 3a.

Fitting a linear model by regressing Lab on Field as;

```{r, warning=FALSE, message=FALSE}

fit_lm <- lm( Lab ~ Field, data = pipeline)



plot(fit_lm, which = 1)

```

We can see from the residual vs fitted plot that linearity assumption is checked but the residuals have a funnel like pattern. As x increases the variability increases.

### 3b

```{r, warning=FALSE, message=FALSE}

i = order(pipeline$Field)
npipe = pipeline[i,]
ff = gl(12,9)[-108]
meanfield = unlist(lapply(split(npipe$Field,ff),mean))
varlab = unlist(lapply(split(npipe$Lab,ff),var))
```


```{r, warning=FALSE, message=FALSE}

# Remove last point
meanfield <- meanfield[-length(meanfield)]
varlab <- varlab[-length(varlab)]

#  Log transform
log_meanfield <- log(meanfield)
log_varlab <- log(varlab)

#  model
var_model <- lm(log_varlab ~ log_meanfield)
tidy_var <- tidy(var_model)
kable(tidy_var, caption = "Log Transform Estimates")

# Extract coefficients for a0 and a1
## take exp because of the log 
a0 <- exp(coef(var_model)[1]) 

## slope - a1
a1 <- coef(var_model)[2]       

#   WLS as the inverse 
predicted_variance <- a0 * (pipeline$Field ^ a1)
weights <- 1 / predicted_variance

# Perform WLS regression of Lab on Field using the calculated weights
wls_model <- lm(Lab ~ Field, data = pipeline, weights = weights)
tidy_wls <- tidy(wls_model)
kable(tidy_wls, caption = "Calculated Weights")

```


## 3c

```{r, warning=FALSE, message=FALSE, eval=FALSE, include=FALSE}

# Original Plot
plot(pipeline$Field, pipeline$Lab, main = "Original Plot of Lab vs Field", 
     xlab = "Field", ylab = "Lab")

# Square Root Transformations

sqrt_lab_model <- lm(sqrt(Lab) ~ Field, data = pipeline) #fit model


plot(pipeline$Field, sqrt(pipeline$Lab), main = "Square Root Transformation of Lab vs Field",  xlab = "Field", ylab = "sqrt(Lab)")
abline(sqrt_lab_model, col = "blue")


# Square root of Field vs Lab
sqrt_field_model <- lm(Lab ~ sqrt(Field), data = pipeline) # fit model

plot(sqrt(pipeline$Field), pipeline$Lab, main = "Square Root Transformation of Field vs Lab", xlab = "sqrt(Field)", ylab = "Lab")
abline(sqrt_field_model, col = "blue")



# Log Transformations

log_lab_model <- lm(log(Lab) ~ Field, data = pipeline) #fit model

plot(pipeline$Field, log(pipeline$Lab), main = "Log Transformation of Lab vs Field",
     xlab = "Field", ylab = "log(Lab)")
abline(log_lab_model, col = "blue")





# Log of Field vs Lab
log_field_model <- lm(Lab ~ log(Field), data = pipeline) #fit model

plot(log(pipeline$Field), pipeline$Lab, main = "Log Transformation of Field vs Lab",
     xlab = "log(Field)", ylab = "Lab")
abline(log_field_model, col = "blue")




# 3. Inverse Transformations
inv_lab_model <- lm(I(1/Lab) ~ Field, data = pipeline) # fit model

plot(pipeline$Field, 1/pipeline$Lab, main = "Inverse Transformation of Lab vs Field",
     xlab = "Field", ylab = "1/Lab")
abline(inv_lab_model, col = "blue")


# Inverse of Field vs Lab
inv_field_model <- lm(Lab ~ I(1/Field), data = pipeline) # fit model

plot(1/pipeline$Field, pipeline$Lab, main = "Inverse Transformation of Field vs Lab",
     xlab = "1/Field", ylab = "Lab")
abline(inv_field_model, col = "blue")


par(mfrow = c(1,1))

# Diagnostic plots for sqrt(Lab) vs Field
plot(sqrt_lab_model, which = 1, main = "Residuals vs Fitted (sqrt(Lab))" )

# Diagnostic plots for Lab vs sqrt(Field)

plot(sqrt_field_model, which = 1, main = "Residuals vs Fitted (sqrt(Field))")

# Diagnostic plots for log(Lab) vs Field

plot(log_lab_model, which = 1,  main = "Residuals vs Fitted (log(Lab))")

# Diagnostic plots for Lab vs log(Field)

plot(log_field_model, which = 1, main = "Residuals vs Fitted (log(Field))")

# Diagnostic plots for 1/Lab vs Field

plot(inv_lab_model, which = 1, main = "Residuals vs Fitted (1/Lab)")

# Diagnostic plots for Lab vs 1/Field

plot(inv_field_model, which = 1,  main = "Residuals vs Fitted (1/Field)")




```




```{r, warning=FALSE, message=FALSE, fig.height = 7, fig.width= 6}

## function to automate 
linear_const_var_trans <- function(Lab_trans, Field_trans, label) {
  
#  model with specified transformations
  model <- lm(Lab_trans ~  Field_trans)
 
# Plot data and model diagnostics
  par(mfrow = c(2, 2))
  plot(Field_trans, Lab_trans, main = paste("Scatter:", label),
       xlab = "Transformed Field", ylab = "Transformed Lab")
  abline(model, col = "red")
 
  # Plot diagnostics
  plot(model, which = 1:2, main = paste("Diagnostics:", label))
  tidy_model <- tidy(model)
  kable(tidy_model, caption = "Summary of Model", digits = 4)
}

# Different 3 transformations given 
sqrt_lab <- sqrt(pipeline$Lab)
sqrt_field <- sqrt(pipeline$Field)
log_lab <- log(pipeline$Lab)
log_field <- log(pipeline$Field)
inv_lab <- 1 / pipeline$Lab
inv_field <- 1 / pipeline$Field

# Apply transformations and evaluate each
linear_const_var_trans(pipeline$Lab,sqrt_field, "Lab vs Sqrt(Field)")
linear_const_var_trans(log_lab,pipeline$Field, "Log Lab vs Field")
linear_const_var_trans(sqrt_lab, sqrt_field, "Square Root Transformation")
linear_const_var_trans(log_lab, log_field, "Log Transformation")
linear_const_var_trans(inv_lab, inv_field, "Inverse Transformation")
linear_const_var_trans(sqrt_lab, log_field, "Sqrt(Lab), Log(Field)")
linear_const_var_trans(log_lab, sqrt_field, "Log(Lab), Sqrt(Field)")

```


Based on these diagnostics plots, the best transformation which shows a linear relationship, no clear pattern and with constant variance is the log transformation.

The Log transformation on both lab and field is the most effective with smallest residual error and also the range scale on the plots is very small.

Inverse transformation also had small range on the residual plot and is less variable.

Square root transforms performs poorly.